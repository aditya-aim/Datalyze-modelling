# ğŸ” AI Age Estimation Error Analysis ğŸ”

## ğŸ‘‹ Welcome!
Hello there! Welcome to this interactive exploration of age estimation errors in AI systems. We'll be analyzing how factors like image quality and lighting conditions affect prediction accuracy. Get ready for some fascinating insights!

## ğŸ“š Step 1: Import Libraries
Let's start by importing all the tools we'll need:

```python
import os
import pandas as pd
import numpy as np
import joblib
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.svm import SVR
from xgboost import XGBRegressor
```

These libraries will help us with:
- Data manipulation (Pandas, NumPy)
- Visualization (Matplotlib, Seaborn)
- Machine learning (Scikit-learn, XGBoost)
- Model persistence (Joblib)

## ğŸ—‚ï¸ Step 2: Load the Dataset
Let's load our age estimation dataset:

```python
df = pd.read_csv("d4.AI_Age_Estimation_Error_Analysis.csv")
```

### Dataset Overview:
This dataset contains:
- **Actual_Age**: The true age of individuals
- **Predicted_Age**: What the AI system estimated
- **Image_Quality_Score**: Quality metric of the input image
- **Lighting_Condition**: Categorical variable (Bright/Dim/Moderate)
- **Prediction_Error**: Our target variable (difference between actual and predicted age)

Dataset Info:
```
RangeIndex: 1000 entries, 0 to 999
Data columns (total 6 columns):
 #   Column               Non-Null Count  Dtype 
---  ------               --------------  ----- 
 0   User_ID              1000 non-null   object
 1   Actual_Age           1000 non-null   int64 
 2   Predicted_Age        1000 non-null   int64 
 3   Image_Quality_Score  1000 non-null   int64 
 4   Lighting_Condition   1000 non-null   object
 5   Prediction_Error     1000 non-null   int64 
```

## ğŸ” Step 3: Basic Exploration
Let's understand our data better:

```python
print("ğŸ“Š Summary Statistics:")
print(df.describe())

print("\nğŸ” Checking for Null Values:")
print(df.isnull().sum())

print("\nğŸ¯ Unique values in Lighting_Condition:")
print(df['Lighting_Condition'].unique())
```

Key Statistics:
```
        Actual_Age  Predicted_Age  Image_Quality_Score  Prediction_Error
count  1000.000000    1000.000000          1000.000000       1000.000000
mean     49.130000      49.221000            56.657000         14.243000
std      18.438956      22.177511            15.778286          8.493179
```

Initial Observations:
- No missing values - great!
- Prediction errors range from 0 to 36 years
- Three lighting conditions: Bright, Dim, Moderate

## ğŸ“ˆ Step 4: Data Visualization
Let's visualize our data distributions:

```python
plt.figure(figsize=(14, 10))
features = ['Actual_Age', 'Predicted_Age', 'Image_Quality_Score', 'Prediction_Error']
for i, feature in enumerate(features):
    plt.subplot(2, 2, i+1)
    sns.histplot(df[feature], kde=True, bins=30, color='blue')
    plt.title(f'Distribution of {feature}')
plt.tight_layout()
plt.show()
```

![4 PLOT1](https://github.com/user-attachments/assets/82b54854-7b77-41b4-8384-8eed22ca5eae)


Key Insights:
- Prediction errors are roughly normally distributed
- Image quality scores are left-skewed
- Both actual and predicted ages cover the full adult range (18-80)

## ğŸ”— Step 5: Correlation Analysis
Let's examine how features relate to prediction error:

```python
correlation_matrix = df[features].corr()
print("\nğŸ”— Correlations with Prediction_Error:")
print(correlation_matrix['Prediction_Error'])

plt.figure(figsize=(10, 6))
sns.barplot(x=correlations.index, y=correlations.values, palette='coolwarm')
plt.title('Feature Correlation with Prediction_Error')
plt.ylabel('Correlation Coefficient')
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()
```
![4 PLOT3](https://github.com/user-attachments/assets/d1594710-c21a-4e13-ba78-b18145e8b3db)

Correlation Results:
```
Actual_Age             0.045442
Predicted_Age          0.038956
Image_Quality_Score   -0.635424
Prediction_Error       1.000000
```

Key Findings:
- Image quality has strong negative correlation with error (better quality â†’ smaller errors)
- Lighting conditions also show meaningful impact (as we'll see after encoding)

## ğŸ› ï¸ Step 6: Feature Engineering
Let's prepare our features for modeling:

```python
# One-Hot Encoding for Lighting Condition
df_encoded = pd.get_dummies(df[['Lighting_Condition']], drop_first=True)
df = pd.concat([df, df_encoded], axis=1)

# Show encoded features
print("\nğŸ¯ Encoded Lighting Conditions:")
print(df_encoded.head())
```

Encoded Features:
```
   Lighting_Condition_Dim  Lighting_Condition_Moderate
0                      0                            0
1                      0                            1
2                      1                            0
3                      0                            0
4                      1                            0
```

## ğŸ§¬ Step 7: Feature Selection â€” Everything Stays!
After thorough analysis, we're keeping all features as each contributes uniquely:

### Features Being Used:

| Feature | Type | Why It's Important |
|---------|------|---------------------|
| Actual_Age | Numerical | Baseline for error calculation |
| Predicted_Age | Numerical | Direct error component |
| Image_Quality_Score | Numerical | Strong error predictor |
| Lighting_Condition | Categorical | Significant impact on errors |


> **Decision:** All features stay as each brings valuable predictive power!

## ğŸ”§ Step 8: Data Preprocessing
Setting up our preprocessing pipeline:

```python
# Define features and target
X = df[['Actual_Age', 'Predicted_Age', 'Image_Quality_Score', 'Lighting_Condition']]
y = df['Prediction_Error']

# Preprocessing steps
numeric_features = ['Actual_Age', 'Predicted_Age', 'Image_Quality_Score']
categorical_features = ['Lighting_Condition']

preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_features),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)
    ]
)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

What's Happening:
- Numerical features are being standardized
- Categorical features are one-hot encoded
- Data is split into 80% training and 20% testing sets

## ğŸ¯ Step 9: Model Selection & Training
We'll compare these regression models:

### Selected Models:

| Model | Why Chosen |
|-------|------------|
| Linear Regression | Simple baseline |
| Random Forest | Handles non-linear relationships well |
| SVR | Good for high-dimensional spaces |
| XGBoost | State-of-the-art performance |


Here's the modified version tailored for your AI Age Estimation Error Analysis project:

### ğŸ”® **Step 10: Coming Up Next**

We're now ready to **train our selected regression models** â€” Linear Regression, Random Forest, SVR, and XGBoost â€” and evaluate how well they predict age estimation errors.

We'll use a comprehensive set of **regression metrics** to assess performance:
- ğŸ“‰ **MAE (Mean Absolute Error)** â€“ Easy-to-interpret average error magnitude
- ğŸ“ **RMSE (Root Mean Squared Error)** â€“ Penalizes larger errors more heavily
- ğŸ” **RÂ² Score** â€“ Measures how well the model explains variance in the data

We'll also leverage powerful visual diagnostics:
- ğŸ“Š **Predicted vs Actual Plots** â€“ To visualize model accuracy across different age ranges
- ğŸ“ˆ **Residual Plots** â€“ For identifying patterns in prediction errors
- ğŸŒŸ **Feature Importance Charts** â€“ To understand what drives model predictions

> ğŸ” Remember, modeling is an iterative process! If initial results need improvement, you'll be able to:
> - ğŸ›ï¸ Fine-tune hyperparameters using grid search
> - ğŸ” Make predictions on new test cases
> - ğŸ”„ Compare performance across different model versions

---

### ğŸ’¬ Final Thoughts:

This is where our data truly comes to life. âœ¨  
Through modeling, we'll transform raw numbers into actionable insights about AI system performance. By comparing multiple approaches, we'll:

1. Identify which factors most influence age estimation accuracy
2. Quantify how much image quality and lighting conditions affect errors
3. Build a predictive tool that could help improve AI systems

You're about to bridge the gap between analysis and real-world impact. The models we develop could ultimately help:
- ğŸ–¼ï¸ Improve image processing algorithms
- ğŸ’¡ Optimize camera settings for age estimation
- ğŸ§  Enhance AI training protocols
