
# Step 0: Intro

# Welcome!

Hey there! ğŸ‘‹ Welcome to this interactive walkthrough of a fun and realistic dataset involving **subscription fees, customer demographics, and card categories**.

I'll be your friendly data science guide today â€” so grab your coffee (or tea â˜•), and let's dive into this step-by-step together like we're exploring it live!

---

## Step 1: Loading & Exploring the Data

We're starting off by importing some essentials: `pandas` and `numpy`, and then reading in the dataset.

```python
import pandas as pd
import numpy as np

# Load dataset
df = pd.read_csv("realistic_subscription_data.csv")

# Show basic info
print("Shape of dataset:", df.shape)
print("\nColumn names:", df.columns.tolist())
print("\nData types:")
print(df.dtypes)

# Show first few rows
print("\nHead of dataset:")
print(df.head())
```

### âœ… Summary:
- We're working with **1,000 rows** and **7 columns**
- It includes details like customer name, age, income, card category, subscription fee, and transaction dates

### ğŸ§ Output: Sample Rows

| Customer Name     | Date       | Month | Card Category     | Customer Age | Income | Subscription Fee |
|------------------|------------|-------|-------------------|--------------|--------|------------------|
| Allison Hill     | 2023-05-02 | 4     | Platinum Rewards  | 45           | 11097  | 106              |
| Noah Rhodes      | 2023-05-11 | 2     | Basic             | 33           | 2871   | 33               |

> **Looking at the dataset**, I noticed that the `Month` column may not match the month in the `Date` field â€” we'll explore that soon!

---

## Step 2: Data Cleaning & Feature Engineering

We're checking for missing values, converting date strings into `datetime`, and extracting helpful features from it like year, weekday, etc.

```python
print("Missing values:\n", df.isnull().sum())

df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
print("\nNumber of invalid dates:", df['Date'].isna().sum())

df['Year'] = df['Date'].dt.year
df['Month_Extracted'] = df['Date'].dt.month
df['Weekday'] = df['Date'].dt.day_name()

print("\nSample rows after datetime processing:")
print(df[['Date', 'Year', 'Month', 'Month_Extracted', 'Weekday']].head())
```

### ğŸ” Output: Date Parsing and Features

| Date       | Year | Month | Month_Extracted | Weekday  |
|------------|------|-------|------------------|----------|
| 2023-05-02 | 2023 | 4     | 5                | Tuesday  |

> ğŸ§  **One interesting thing that stood out**: The `Month` column doesn't always match the actual date's month. This could be a data issue or a mislabeling. Good to keep an eye on this!

---

## Step 3: Distributions & Descriptive Statistics

Time to look at the spread of key numerical variables!

```python
import matplotlib.pyplot as plt
import seaborn as sns

# Basic stats
print(df[['Customer Age', 'Income', 'Subscription Fee']].describe())

# Histograms
fig, axs = plt.subplots(1, 3, figsize=(18, 5))
sns.histplot(df['Customer Age'], kde=True, ax=axs[0], bins=20)
axs[0].set_title('Distribution of Customer Age')

sns.histplot(df['Income'], kde=True, ax=axs[1], bins=30)
axs[1].set_title('Distribution of Income')

sns.histplot(df['Subscription Fee'], kde=True, ax=axs[2], bins=20)
axs[2].set_title('Distribution of Subscription Fee')

plt.tight_layout()
plt.show()
```

### ğŸ“ˆ Output: Summary Stats

![Distribution plots](https://github.com/user-attachments/assets/61850f19-e39e-4eda-ac52-b9d6be81e08d)

- **Mean Age**: 37
- **Mean Income**: 6671
- **Mean Subscription Fee**: ~$60

> ğŸ‘€ **Let's pause and look at** these distributions: most incomes cluster below 10K, and subscription fees seem to vary widely, suggesting tiered services.

---

## Step 4: Categorical Features

Let's explore how card types and weekdays affect subscription behavior.

```python
print("Card Categories:", df['Card Category'].unique())
print("\nCard Category Counts:\n", df['Card Category'].value_counts())

sns.countplot(data=df, x='Card Category', order=df['Card Category'].value_counts().index)
plt.title('Distribution of Card Categories')
plt.xticks(rotation=15)
plt.show()

print("\nWeekday Counts:\n", df['Weekday'].value_counts())

sns.countplot(data=df, x='Weekday', order=df['Weekday'].value_counts().index)
plt.title('Transactions by Weekday')
plt.show()
```

### ğŸ§¾ Output: Category Counts
![image (5)](https://github.com/user-attachments/assets/5251c4b9-984d-40e6-86d5-5ca6a114fd91)


- **Top Card**: Silver (272 users)
- **Top Weekday**: Friday, followed closely by Sunday and Saturday

> ğŸ’¡ **One thing to watch out for here is** the balance in card types â€” this might affect model training if we were to predict card behavior.

---

## Step 5: Outlier Detection

Let's use IQR (Interquartile Range) to find outliers in age, income, and fee.

```python
def detect_outliers_iqr(column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower = Q1 - 1.5 * IQR
    upper = Q3 + 1.5 * IQR
    outliers = df[(df[column] < lower) | (df[column] > upper)]
    return len(outliers), lower, upper

for col in ['Customer Age', 'Income', 'Subscription Fee']:
    count, lower, upper = detect_outliers_iqr(col)
    print(f"{col} -> Outliers: {count}, Lower Bound: {lower:.2f}, Upper Bound: {upper:.2f}")
```

### âš ï¸ Output: Outliers

- **Customer Age**: 11 outliers (older folks)
- **Income & Subscription Fee**: No extreme outliers

> âœ… That's a relief â€” most of our numerical features look well-distributed!

---

## Step 6: Correlation Analysis

```python
sns.heatmap(df[['Customer Age', 'Income', 'Subscription Fee']].corr(), annot=True, cmap='coolwarm')
plt.title('Correlation Between Numerical Features')
plt.show()
```

### ğŸ” Output: Correlation Heatmap

![Correlation Heatmap](https://github.com/user-attachments/assets/1cc12667-1be2-41d9-ab57-60817a36f814)

> ğŸ” Not much correlation between age and income, but **subscription fee has a modest positive correlation with income**, which makes intuitive sense.

---

## Step 7: Feature Importances Visualization

### Goal:
Understand how numerical features (`Income`, `Customer Age`) correlate with `Subscription Fee` using **statistics + visualizations**.

```python
import seaborn as sns
import matplotlib.pyplot as plt

# Calculate correlations
correlation_matrix = df[['Customer Age', 'Income', 'Subscription Fee']].corr()
target_corr = correlation_matrix['Subscription Fee'].sort_values(ascending=False)

print("Correlation with Subscription Fee:\n", target_corr)

# Plot 1: Bar plot of correlations
plt.figure(figsize=(8, 4))
sns.barplot(x=target_corr.index, y=target_corr.values, palette="Blues_d")
plt.title("Correlation with Subscription Fee")
plt.ylim(-0.1, 1.0)
plt.ylabel("Pearson Correlation Coefficient")
plt.axhline(y=0, color='black', linestyle='--')
plt.show()

# Plot 2: Scatter plots
fig, axes = plt.subplots(1, 2, figsize=(12, 5))
sns.scatterplot(data=df, x='Income', y='Subscription Fee', ax=axes[0], alpha=0.6, color='blue')
axes[0].set_title("Income vs. Subscription Fee (Corr = 0.93)")
sns.scatterplot(data=df, x='Customer Age', y='Subscription Fee', ax=axes[1], alpha=0.6, color='green')
axes[1].set_title("Age vs. Subscription Fee (Corr = 0.16)")
plt.tight_layout()
plt.show()
```

### ğŸ“Š Output & Interpretation

#### Numerical Correlation Coefficients
```
Correlation with Subscription Fee:
Subscription Fee    1.000000
Income              0.933665
Customer Age        0.159394
Name: Subscription Fee, dtype: float64
```

#### ğŸ’¡ Key Insight:
- `Income` has a **very strong positive correlation (0.93)** with subscription fees.
- `Age` shows a **weak relationship (0.16)**, suggesting it's a minor factor.

#### Bar Plot: Correlation Strength
![Correlation Bar Plot](https://github.com/user-attachments/assets/3af00a23-19e5-4017-bf7f-687ce80223c8)

- `Income` dominates the predictive power.

#### Scatter Plots: Visual Trends
![Scatter Plots](https://github.com/user-attachments/assets/631ab1d7-d734-4201-b420-d6f4a930237e)

**Left (Income):**
- Clear upward trend: Higher income â†’ Higher fees.

**Right (Age):**
- Weak trend with high variance: Age alone isn't decisive.

#### ğŸ¯ Actionable Takeaways
1. **Focus on `Income`**: Strongest driver of subscription fees.
2. **Question `Age`**: Weak correlationâ€”consider engineering age groups (e.g., "Young/Middle/Senior") for better signal.
3. **Next Step**: Use this insight to prioritize features for modeling (e.g., drop `Age` or transform it).

---

## Step 8: Feature Selection â€” *You're in Control!*

Now that we've engineered some cool features, it's time to zoom in on the **most important ones** â€” but here's the twist: **you get to decide** how many!

### ğŸ‘‰ Why Feature Selection?
Feature selection helps:
- Remove irrelevant/noisy data
- Reduce overfitting
- Speed up training
- Improve model performance ğŸš€

![Feature Selection](https://github.com/user-attachments/assets/e7c17efa-8969-4812-93e5-1ac310d9a652)

> **Nice!** These are the features that statistically have the strongest relationship with the subscription fee tier. This helps reduce noise and improve model performance!

---

## Step 9: Feature Engineering

This is where the magic starts! We're creating dummies, weekend flags, and age groups.

```python
df = pd.get_dummies(df, columns=['Card Category', 'Weekday'], drop_first=True)

df['Is_Weekend'] = df['Date'].dt.weekday.isin([5, 6]).astype(int)



# Age groups
df['Age_Group'] = pd.cut(df['Customer Age'], bins=[20, 30, 40, 50, 60, 70], labels=['20s', '30s', '40s', '50s', '60s'], right=False)

# Let's show a quick preview:
print("\nData with New Features:")
print(df.head())
```

---

### Final Thoughts

- Clean data + useful transformations = cleaner models!
- We've laid down the groundwork for a predictive model that could be used to forecast subscription fees or even segment customers.


Sure! Here's the continuation of the "content" in Markdown format:


---

## Step 10: Predictive Modeling â€” Let's Build a Model! ğŸ—ï¸

Now that we've explored and engineered features, itâ€™s time to build a predictive model. Let's predict **Subscription Fee** using our cleaned and transformed data.

After analyzing the dataset, weâ€™ve identified 8 possible models to consider for the regression task. These models cover a range of techniques to capture various patterns in the data and address both linear and non-linear relationships.

### Possible Models:
- **Random Forest Regressor** ğŸŒ²: Captures complex non-linear relationships.
- **Gradient Boosting Regressor** ğŸ”¥: Handles intricate data interactions for improved performance.
- **Logistic Regression** ğŸ“‰: Tests for linear trends and serves as a baseline model.
- **SVM Regressor** ğŸ§©: Captures non-linear relationships, especially with high-dimensional data.
- **K-Nearest Neighbors (KNN)** ğŸ”: Useful for capturing local patterns.
- **Linear Regression** ğŸ“Š: Simple baseline for comparison.
- **XGBoost** âš¡: Advanced version of Gradient Boosting for high accuracy.
- **Subscription Change Predictor** ğŸ”„: A classification model predicting behavior (upgrade/downgrade).

---
![Feature Selection](https://github.com/user-attachments/assets/e7c17efa-8969-4812-93e5-1ac310d9a652)


### **Final Model Selection:**

After reviewing the performance and relevance of each model, weâ€™ve decided to proceed with the following subset:

```python
model_names = ['RandomForest_model', 'GradientBoosting_model', 'LogisticRegression_model', 'SVM_model', 'subscription_change_predictor']
```

These models have been selected based on the following reasoning:

1. **Random Forest Regressor** ğŸŒ²  
   *Reasoning*: Random Forest is robust to overfitting and excels at capturing complex, non-linear relationships in data. Itâ€™s also effective with high-dimensional datasets and can handle a variety of features.

2. **Gradient Boosting Regressor** ğŸ”¥  
   *Reasoning*: Gradient Boosting is known for its ability to improve model accuracy by iteratively correcting errors from previous trees. It works well for capturing intricate patterns and is highly effective in prediction tasks.

3. **Logistic Regression** ğŸ“‰  
   *Reasoning*: Despite being a simple model, Logistic Regression serves as a good baseline. It helps check for linear relationships and can offer insights into simpler trends or decision boundaries in the data.

4. **Support Vector Machine (SVM) Regressor** ğŸ§©  
   *Reasoning*: SVM is effective in high-dimensional spaces and can capture both linear and non-linear trends, making it useful for datasets with complex interactions between features.

5. **Subscription Change Predictor** ğŸ”„  
   *Reasoning*: This model is designed to shift the task from predicting exact fee amounts to predicting customer behavior, such as upgrades or downgrades. It may uncover valuable insights for business decisions based on customer behavior.




## Step 11: Conclusion & Next Steps âœ…

Great job! ğŸ‰ You've successfully worked through the core steps of **exploring**, **cleaning**, and **engineering** the dataset. With these foundations laid, we're now set to move into the exciting phase of **model training and evaluation**. ğŸš€

---

### ğŸ§  Coming Up Next:

Weâ€™ll proceed to **train the selected models** and **evaluate their performance** using key **regression and classification metrics** (like RMSE, RÂ², accuracy, precision, recall) along with visual tools such as the **confusion matrix**.

ğŸ”„ If the initial results arenâ€™t satisfactory, donâ€™t worry â€” youâ€™ll have the option to **fine-tune** the models or **experiment with others** from our broader pool.

Youâ€™ll also be able to:
- ğŸ” Make predictions on new data
- ğŸ”§ Adjust hyperparameters
- ğŸ”„ Make predictions on the tuned models

---

### ğŸ’¬ Final Thoughts:

This stage of the workflow is where insights come to life. By comparing model performances, we can determine what works best for our business use case.

You're now just a few steps away from delivering a predictive solution that could drive real-world impact. Letâ€™s keep going! ğŸ

---

