
---

# 🕵️ Hello brilliant data detective!

Welcome back to **DataLyze**, your personal AI-powered data scientist. Today, we’re diving deep into a **subscription-based customer dataset** to build a classification model that can predict **card categories** based on customer behavior and demographics.

Let’s roll up our sleeves and tackle this step by step, with some friendly guidance, smart reasoning, and of course — powerful code.

---

## 🧾 Step 0: Dataset Overview

Before jumping into the code, let’s understand the **structure** and **goal** of our dataset.

From our first look, this dataset seems to be all about **subscription services**. Here’s what the key features tell us:

- `Customer Age`: The age of the customer.
- `Income`: Likely the annual income.
- `Subscription Fee`: The amount paid — could indicate tier level.
- `Date`: When this customer interaction or record was logged.
- `Card Category`: This is our **target variable** — the label we want to predict.

So our goal?  
🎯 **Predict the customer’s Card Category** (e.g., Silver, Gold, etc.) using the available features.

---

## 🧹 Step 1: Load and Peek at the Data

Let’s load the data and get a quick sense of what we’re working with.

```python
import pandas as pd

df = pd.read_csv("realistic_subscription_data.csv")
df.head()
```

👀 *This gives us the first 5 rows to visually inspect the dataset.*

---

## 🛠️ Step 2: Check Data Types & Missing Values

Before any transformation, we need to know the datatype of each column and whether there are any missing values to fix.

```python
df.info()
df.isnull().sum()
```

💡 *Why?*  
Missing values can throw off model training, and understanding column types tells us what preprocessing steps we’ll need later (e.g., encoding, datetime parsing, etc.).

---

## 🔍 Step 3: Inspect the Target Variable (`Card Category`)

Let’s check how many unique categories we’re trying to predict and if the dataset is balanced across them.

```python
df['Card Category'].value_counts()
```

🧠 *Insight:*  
This helps us confirm how many classes we’re predicting — and if some are underrepresented, we’ll know to consider balancing later.

---

## 📆 Step 4: Convert `Date` and Extract Time-Based Features

Time to convert the `Date` column into a proper datetime object and extract **month** and **year**, which can give seasonal or trend-related signals to our model.

```python
df['Date'] = pd.to_datetime(df['Date'])
df['Year'] = df['Date'].dt.year
df['Month'] = df['Date'].dt.month
df.drop(columns=['Date'], inplace=True)
```

⏳ *Reason:*  
Dates hold temporal patterns. Knowing **when** something happened can be as important as **what** happened.

---

## 🏷️ Step 5: Encode the Target Column

Since `Card Category` is a string, let’s turn it into numbers using Label Encoding.

```python
from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
df['Card Category'] = label_encoder.fit_transform(df['Card Category'])
df['Card Category'].value_counts()
```

✨ Now our target is numerical — ready for machine learning.

---

## 📂 Step 6: Split Features and Target

Let’s separate the dataset into features (inputs) and the target (output).

```python
X = df.drop(columns=["Card Category"])
y = df["Card Category"]
```

---

## 🔀 Step 7: Train-Test Split

We’ll split the dataset into training and testing sets — this helps evaluate our model fairly on unseen data.

```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
```

📌 *Note:* 80% for training and 20% for testing is a good default.

---

## 🤖 Step 8: Train Multiple Classification Models

Let’s try a few popular models to see which performs best. We'll train:

- Random Forest
- Gradient Boosting
- Logistic Regression
- Support Vector Machine (SVM)

```python
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

models = {
    "Random Forest": RandomForestClassifier(),
    "Gradient Boosting": GradientBoostingClassifier(),
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "SVM": SVC()
}

accuracies = {}

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    accuracies[name] = acc
    print(f"{name} Accuracy: {acc:.4f}")
```

⚖️ *We’re evaluating on test accuracy to see who’s the top performer.*

---

## 🏁 Step 9: Pick the Best Model

Time to reveal the winner!

```python
best_model_name = max(accuracies, key=accuracies.get)
best_model = models[best_model_name]

print(f"\n✅ Best Performing Model: {best_model_name} with Accuracy: {accuracies[best_model_name]:.4f}")
```

🏆 *Nice! We now have the top model from our mini tournament.*

---

## 💾 Step 10: Save the Best Model (Optional)

If you’d like to save this model for reuse later (e.g., in your Streamlit app or API):

```python
import joblib

joblib.dump(best_model, f"{best_model_name.replace(' ', '_')}.pkl")
```

---

## 💡 What’s Next?

🎉 That wraps up our **end-to-end preprocessing and model training journey** inside **DataLyze**!

But we’re not stopping here — here’s what you can do next:

1. **Make Predictions**: Try predicting new customer records using `best_model.predict(new_data)`.
2. **Fine-Tune the Model**: Use techniques like `GridSearchCV`, feature selection, or even ensemble stacking to improve accuracy.
3. **Interpret the Results**: Use SHAP or feature importance plots to understand why your model makes the decisions it does.

---

🔮 **In the next step, you can test your model’s predictions**, evaluate confusion matrices, and maybe even explore **hyperparameter tuning** to squeeze out more accuracy.

Let me know when you’re ready to move to the next step — I’ve got plenty more DataLyze magic ready for you! ✨
