
---

# ğŸ•µï¸ Hello brilliant data detective!

Welcome back to **DataLyze**, your personal AI-powered data scientist. Today, weâ€™re diving deep into a **subscription-based customer dataset** to build a classification model that can predict **card categories** based on customer behavior and demographics.

Letâ€™s roll up our sleeves and tackle this step by step, with some friendly guidance, smart reasoning, and of course â€” powerful code.

---

## ğŸ§¾ Step 0: Dataset Overview

Before jumping into the code, letâ€™s understand the **structure** and **goal** of our dataset.

From our first look, this dataset seems to be all about **subscription services**. Hereâ€™s what the key features tell us:

- `Customer Age`: The age of the customer.
- `Income`: Likely the annual income.
- `Subscription Fee`: The amount paid â€” could indicate tier level.
- `Date`: When this customer interaction or record was logged.
- `Card Category`: This is our **target variable** â€” the label we want to predict.

So our goal?  
ğŸ¯ **Predict the customerâ€™s Card Category** (e.g., Silver, Gold, etc.) using the available features.

---

## ğŸ§¹ Step 1: Load and Peek at the Data

Letâ€™s load the data and get a quick sense of what weâ€™re working with.

```python
import pandas as pd

df = pd.read_csv("realistic_subscription_data.csv")
df.head()
```

ğŸ‘€ *This gives us the first 5 rows to visually inspect the dataset.*

---

## ğŸ› ï¸ Step 2: Check Data Types & Missing Values

Before any transformation, we need to know the datatype of each column and whether there are any missing values to fix.

```python
df.info()
df.isnull().sum()
```

ğŸ’¡ *Why?*  
Missing values can throw off model training, and understanding column types tells us what preprocessing steps weâ€™ll need later (e.g., encoding, datetime parsing, etc.).

---

## ğŸ” Step 3: Inspect the Target Variable (`Card Category`)

Letâ€™s check how many unique categories weâ€™re trying to predict and if the dataset is balanced across them.

```python
df['Card Category'].value_counts()
```

ğŸ§  *Insight:*  
This helps us confirm how many classes weâ€™re predicting â€” and if some are underrepresented, weâ€™ll know to consider balancing later.

---

## ğŸ“† Step 4: Convert `Date` and Extract Time-Based Features

Time to convert the `Date` column into a proper datetime object and extract **month** and **year**, which can give seasonal or trend-related signals to our model.

```python
df['Date'] = pd.to_datetime(df['Date'])
df['Year'] = df['Date'].dt.year
df['Month'] = df['Date'].dt.month
df.drop(columns=['Date'], inplace=True)
```

â³ *Reason:*  
Dates hold temporal patterns. Knowing **when** something happened can be as important as **what** happened.

---

## ğŸ·ï¸ Step 5: Encode the Target Column

Since `Card Category` is a string, letâ€™s turn it into numbers using Label Encoding.

```python
from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
df['Card Category'] = label_encoder.fit_transform(df['Card Category'])
df['Card Category'].value_counts()
```

âœ¨ Now our target is numerical â€” ready for machine learning.

---

## ğŸ“‚ Step 6: Split Features and Target

Letâ€™s separate the dataset into features (inputs) and the target (output).

```python
X = df.drop(columns=["Card Category"])
y = df["Card Category"]
```

---

## ğŸ”€ Step 7: Train-Test Split

Weâ€™ll split the dataset into training and testing sets â€” this helps evaluate our model fairly on unseen data.

```python
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
```

ğŸ“Œ *Note:* 80% for training and 20% for testing is a good default.

---

## ğŸ¤– Step 8: Train Multiple Classification Models

Letâ€™s try a few popular models to see which performs best. We'll train:

- Random Forest
- Gradient Boosting
- Logistic Regression
- Support Vector Machine (SVM)

```python
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

models = {
    "Random Forest": RandomForestClassifier(),
    "Gradient Boosting": GradientBoostingClassifier(),
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "SVM": SVC()
}

accuracies = {}

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    accuracies[name] = acc
    print(f"{name} Accuracy: {acc:.4f}")
```

âš–ï¸ *Weâ€™re evaluating on test accuracy to see whoâ€™s the top performer.*

---

## ğŸ Step 9: Pick the Best Model

Time to reveal the winner!

```python
best_model_name = max(accuracies, key=accuracies.get)
best_model = models[best_model_name]

print(f"\nâœ… Best Performing Model: {best_model_name} with Accuracy: {accuracies[best_model_name]:.4f}")
```

ğŸ† *Nice! We now have the top model from our mini tournament.*

---

## ğŸ’¾ Step 10: Save the Best Model (Optional)

If youâ€™d like to save this model for reuse later (e.g., in your Streamlit app or API):

```python
import joblib

joblib.dump(best_model, f"{best_model_name.replace(' ', '_')}.pkl")
```

---

## ğŸ’¡ Whatâ€™s Next?

ğŸ‰ That wraps up our **end-to-end preprocessing and model training journey** inside **DataLyze**!

But weâ€™re not stopping here â€” hereâ€™s what you can do next:

1. **Make Predictions**: Try predicting new customer records using `best_model.predict(new_data)`.
2. **Fine-Tune the Model**: Use techniques like `GridSearchCV`, feature selection, or even ensemble stacking to improve accuracy.
3. **Interpret the Results**: Use SHAP or feature importance plots to understand why your model makes the decisions it does.

---

ğŸ”® **In the next step, you can test your modelâ€™s predictions**, evaluate confusion matrices, and maybe even explore **hyperparameter tuning** to squeeze out more accuracy.

Let me know when youâ€™re ready to move to the next step â€” Iâ€™ve got plenty more DataLyze magic ready for you! âœ¨
