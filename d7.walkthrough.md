

### ğŸ—‚ï¸ **Step 0: Introduction**

Hey there, credit card insights explorer! ğŸ’³âœ¨  
We're about to dive into an intriguing dataset tracking credit card transactions to predict **high-spending customers**. Think of me as your data science co-pilotâ€”we'll explore, visualize, and model this together just like a real Jupyter Notebook session. Let's get started!

---

### ğŸ—‚ï¸ **Step 1: Loading the Dataset**

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
data = pd.read_csv('d7.Decline_Credit_Card_Spending.csv')

# View the first 5 rows
data.head()
```

#### âœ… Output: Sample Rows
| Transaction Date | Customer Age Group | Transaction Amount | Category | Reward Points Earned | Transaction Frequency | Quarter No |
|------------------|--------------------|--------------------|----------|----------------------|-----------------------|------------|
| 2023-04-13       | 18-25              | 18.45              | Groceries| 13                   | 1                     | Q2         |
| 2023-12-15       | <18                | 35.19              | Entertainment | 4                | 15                    | Q4         |

---

### ğŸ”§ **Step 2: Data Preprocessing**

```python
# Convert date and create target variable
data['Transaction Date'] = pd.to_datetime(data['Transaction Date'])
median_spend = data['Transaction Amount'].median()
data['High Spender'] = (data['Transaction Amount'] > median_spend).astype(int)

# One-hot encode categoricals
data_encoded = pd.get_dummies(data, columns=['Customer Age Group', 'Category', 'Quarter No'], drop_first=True)

# Check missing values
missing_values = data_encoded.isnull().sum()
```

#### âœ… Key Actions:
- Created binary target `High Spender` (1 = above median transaction amount)
- Encoded categoricals like `Customer Age Group_26-40`, `Category_Travel`
- **Zero missing values** - perfect! ğŸ¯

---

### ğŸ“Š **Step 3: Target & Feature Distributions**

```python
# Plot transaction amounts
plt.figure(figsize=(10,6))
sns.histplot(data['Transaction Amount'], kde=True, bins=30)
plt.title('Transaction Amount Distribution');
```
![7 PLOT1](https://github.com/user-attachments/assets/4f326bb7-bc0a-4887-9d79-8d6c34a6d5a3)


```python
# Target balance check
sns.countplot(x='High Spender', data=data_encoded)
print(data_encoded['High Spender'].value_counts())
```

#### âœ… Output:
```
0    2500  # Below median spenders
1    2500  # High spenders
```
**Perfectly balanced dataset!** No need for imbalance handling techniques.

---

### ğŸ”¥ **Step 4: Feature Correlation with Target**

```python
# Calculate correlations
corr = data_encoded.corr()['High Spender'].drop('High Spender').sort_values()

# Visualization
plt.figure(figsize=(10,6))
sns.barplot(x=corr.values, y=corr.index, palette="viridis")
plt.title("Feature Correlation with High Spender");
```

![7 PLOT3](https://github.com/user-attachments/assets/36828523-f000-46c8-830f-7272a9feea31)

#### ğŸ’¡ Key Insights:
- **Transaction Amount**: Ultra-strong correlation (`0.82`) - Makes sense since it directly defines our target!
- **Transaction Frequency**: Moderate correlation (`0.43`) - Frequent spenders tend to be high spenders
- **Age Groups**: 
  - `41+` shows positive correlation (`0.26`) - Older customers spend more
  - `<18` surprisingly correlates positively (`0.11`) - Teens make some high transactions!
- **Categories**: Groceries has slight positive correlation (`0.08`)

> ğŸš¨ **Red Flag**: The extreme correlation with Transaction Amount suggests potential **target leakage** - We might need to exclude this feature if predicting *future* high spending!

---

### ğŸ¤– **Step 5: Model Selection Strategy**

Since we're predicting a binary outcome (`High Spender`), here's our game plan:

#### ğŸ† **Selected Models**
| Model | Why We're Choosing It |
|-------|------------------------|
| âœ… **Logistic Regression** | Baseline model - interpretable coefficients |
| âœ… **Random Forest** | Handles non-linear relationships well |
| âœ… **Gradient Boosting** | State-of-the-art for structured data |
| âœ… **SVM** | Good for high-dimensional spaces |

#### ğŸ¯ **Evaluation Metrics**
We'll track:
- **Accuracy** (but careful - balanced dataset!)
- **Precision/Recall/F1** 
- **ROC-AUC** (best for binary classification)

---

### âš™ï¸ **Step 6: Train-Test Split & Scaling**

```python
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

X = data_encoded.drop(columns=['High Spender', 'Transaction Date'])
y = data_encoded['High Spender']

# Stratified split (though balanced)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
```

#### âœ… Output Shapes:
- Training: (4000, 12)
- Test: (1000, 12)

---
ğŸ¯ Step 10: Selected Models  
From the list of possibilities, we narrowed down our first round of modeling to a few that strike a balance between performance, speed, and interpretability, based on the characteristics of our data:
ğŸ§ª List of Possible Models (with Dataset-Specific Reasons)

Model	Reason
Logistic Regression	Great baseline for binary target (High Spender). Works well with numeric, correlated features like Transaction Amount.
KNN	Simple, but may struggle with 12 features and 5000 rows. Sensitive to scaling and high-dimensional data.
Decision Tree	Good for interpreting feature splits (e.g. age or amount), but prone to overfitting alone.
Random Forest	Handles mixed feature types, captures interactions well, and is robust to noise and overfitting.
Gradient Boosting	Ideal for structured, clean data like this. Captures complex relationships with high accuracy.
SVM	Scales well with our standardized features and can find optimal class boundaries.
Naive Bayes	Not ideal here â€” assumes feature independence, which doesn't hold (e.g., amount vs. reward points).
Neural Network	Powerful, but unnecessary complexity for this dataset size and structure. Low interpretability.ğŸ¯ Selected Models  

Model	Why We Picked It (Specific to Our Dataset)
âœ… Logistic Regression	Strong linear correlation between Transaction Amount and High Spender (r = 0.82) makes it a natural baseline. Fast, interpretable, and ideal for binary classification.
âœ… Random Forest	Dataset includes both numerical and one-hot encoded categorical features. Random Forest can handle this mix well and is resistant to overfitting.
âœ… Gradient Boosting	Given clean, structured data and meaningful numeric features (Reward Points, Transaction Frequency), boosting can capture subtle interactions others may miss.
âœ… SVM	After scaling the features, SVM is expected to perform well on our balanced dataset (2500 vs 2500 classes), especially if the decision boundary is non-linear.
ğŸ¯ Selected Models (First Round â€“ Based on Our Dataset)

Model	Why We Picked It (Specific to Our Dataset)
âœ… Logistic Regression	Strong linear correlation between Transaction Amount and High Spender (r = 0.82) makes it a natural baseline. Fast, interpretable, and ideal for binary classification.
âœ… Random Forest	Dataset includes both numerical and one-hot encoded categorical features. Random Forest can handle this mix well and is resistant to overfitting.
âœ… Gradient Boosting	Given clean, structured data and meaningful numeric features (Reward Points, Transaction Frequency), boosting can capture subtle interactions others may miss.
âœ… SVM	After scaling the features, SVM is expected to perform well on our balanced dataset (2500 vs 2500 classes), especially if the decision boundary is non-linear.
