# Credit Card Spending Analysis ðŸš€

## Introduction ðŸ’³

Hey there, credit card insights explorer!  
We're about to dive into an intriguing dataset tracking credit card transactions to predict **high-spending customers**. Think of me as your data science co-pilotâ€”we'll explore, visualize, and model this together just like a real Jupyter Notebook session. Let's get started!

## Step 1: Loading the Dataset ðŸ“Š

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
data = pd.read_csv('d7.Decline_Credit_Card_Spending.csv')

# View the first 5 rows
data.head()
```

### Sample Data Preview
| Transaction Date | Customer Age Group | Transaction Amount | Category | Reward Points Earned | Transaction Frequency | Quarter No |
|------------------|--------------------|--------------------|----------|----------------------|-----------------------|------------|
| 2023-04-13       | 18-25              | 18.45              | Groceries| 13                   | 1                     | Q2         |
| 2023-12-15       | <18                | 35.19              | Entertainment | 4                | 15                    | Q4         |

## Step 2: Data Preprocessing ðŸ”§

```python
# Convert date and create target variable
data['Transaction Date'] = pd.to_datetime(data['Transaction Date'])
median_spend = data['Transaction Amount'].median()
data['High Spender'] = (data['Transaction Amount'] > median_spend).astype(int)

# One-hot encode categoricals
data_encoded = pd.get_dummies(data, columns=['Customer Age Group', 'Category', 'Quarter No'], drop_first=True)

# Check missing values
missing_values = data_encoded.isnull().sum()
```

### Key Actions
- Created binary target `High Spender` (1 = above median transaction amount)
- Encoded categoricals like `Customer Age Group_26-40`, `Category_Travel`
- **Zero missing values** - perfect! ðŸŽ¯

## Step 3: Target & Feature Distributions ðŸ“Š

```python
# Plot transaction amounts
plt.figure(figsize=(10,6))
sns.histplot(data['Transaction Amount'], kde=True, bins=30)
plt.title('Transaction Amount Distribution');
```

![Transaction Amount Distribution](https://github.com/user-attachments/assets/4f326bb7-bc0a-4887-9d79-8d6c34a6d5a3)

```python
# Target balance check
sns.countplot(x='High Spender', data=data_encoded)
print(data_encoded['High Spender'].value_counts())
```

### Output
```
0    2500  # Below median spenders
1    2500  # High spenders
```

**Perfectly balanced dataset!** No need for imbalance handling techniques.

## Step 4: Feature Correlation Analysis ðŸ”¥

```python
# Calculate correlations
corr = data_encoded.corr()['High Spender'].drop('High Spender').sort_values()

# Visualization
plt.figure(figsize=(10,6))
sns.barplot(x=corr.values, y=corr.index, palette="viridis")
plt.title("Feature Correlation with High Spender");
```

![Feature Correlation Plot](https://github.com/user-attachments/assets/36828523-f000-46c8-830f-7272a9feea31)

### Key Insights
- **Transaction Amount**: Ultra-strong correlation (`0.82`) - Makes sense since it directly defines our target!
- **Transaction Frequency**: Moderate correlation (`0.43`) - Frequent spenders tend to be high spenders
- **Age Groups**: 
  - `41+` shows positive correlation (`0.26`) - Older customers spend more
  - `<18` surprisingly correlates positively (`0.11`) - Teens make some high transactions!
- **Categories**: Groceries has slight positive correlation (`0.08`)

> ðŸš¨ **Red Flag**: The extreme correlation with Transaction Amount suggests potential **target leakage** - We might need to exclude this feature if predicting *future* high spending!

## Step 5: Model Selection Strategy ðŸ¤–

Since we're predicting a binary outcome (`High Spender`), here's our game plan:

### Selected Models
| Model | Why We're Choosing It |
|-------|------------------------|
| âœ… **Logistic Regression** | Baseline model - interpretable coefficients |
| âœ… **Random Forest** | Handles non-linear relationships well |
| âœ… **Gradient Boosting** | State-of-the-art for structured data |
| âœ… **SVM** | Good for high-dimensional spaces |

### Evaluation Metrics
We'll track:
- **Accuracy** (but careful - balanced dataset!)
- **Precision/Recall/F1** 
- **ROC-AUC** (best for binary classification)

## Step 6: Train-Test Split & Scaling âš™ï¸

```python
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

X = data_encoded.drop(columns=['High Spender', 'Transaction Date'])
y = data_encoded['High Spender']

# Stratified split (though balanced)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
```

### Output Shapes
- Training: (4000, 12)
- Test: (1000, 12)

## Step 7: Model Selection Analysis ðŸŽ¯

### Considered Models and Their Suitability

| Model | Suitability Analysis |
|-------|----------------------|
| **Logistic Regression** | Great baseline for binary target (High Spender). Works well with numeric, correlated features like Transaction Amount. |
| **KNN** | Simple, but may struggle with 12 features and 5000 rows. Sensitive to scaling and high-dimensional data. |
| **Decision Tree** | Good for interpreting feature splits (e.g. age or amount), but prone to overfitting alone. |
| **Random Forest** | Handles mixed feature types, captures interactions well, and is robust to noise and overfitting. |
| **Gradient Boosting** | Ideal for structured, clean data like this. Captures complex relationships with high accuracy. |
| **SVM** | Scales well with our standardized features and can find optimal class boundaries. |
| **Naive Bayes** | Not ideal here â€” assumes feature independence, which doesn't hold (e.g., amount vs. reward points). |
| **Neural Network** | Powerful, but unnecessary complexity for this dataset size and structure. Low interpretability. |

### Final Model Selection

Based on our analysis, we've selected these models for our first round of modeling:

| Model | Selection Rationale |
|-------|---------------------|
| âœ… **Logistic Regression** | Strong linear correlation between Transaction Amount and High Spender (r = 0.82) makes it a natural baseline. Fast, interpretable, and ideal for binary classification. |
| âœ… **Random Forest** | Dataset includes both numerical and one-hot encoded categorical features. Random Forest can handle this mix well and is resistant to overfitting. |
| âœ… **Gradient Boosting** | Given clean, structured data and meaningful numeric features (Reward Points, Transaction Frequency), boosting can capture subtle interactions others may miss. |
| âœ… **SVM** | After scaling the features, SVM is expected to perform well on our balanced dataset (2500 vs 2500 classes), especially if the decision boundary is non-linear. |

## Coming Up Next ðŸš€

We're now ready to train our selected classification models to predict high-spending customers. Here's what's coming:

### Evaluation Metrics
- Accuracy Score
- Classification Report (Precision/Recall/F1)
- Confusion Matrix


> ðŸ” If initial results are underwhelming, no worries! You'll have the chance to:
> - ðŸ”§ Adjust hyperparameters
> - ðŸ”„ Make predictions on the tuned models
> - ðŸ“ˆ Model Comparison

This modeling phase will help us understand which factors most influence high spending behavior and build a predictive system to support credit card business decisions.

