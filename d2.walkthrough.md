
---

# 🎉 **Welcome to the Data Science Walkthrough!** 🎉

Hey there! 😊 Welcome to this interactive **Jupyter Notebook** session! We're going to explore a dataset, perform some cool data wrangling, and build models together. Think of this as a fun, hands-on data science tutorial where I'll guide you step by step. 💡

## **Step 1: Import Libraries** 📚

Let's kick things off by importing the libraries we need for this project. These libraries will help us load, preprocess, and model the data, plus some tools for visualizing our results.

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import os
```

- **Pandas**: Manages the data tables.
- **NumPy**: Does the math and heavy lifting.
- **Matplotlib & Seaborn**: These are our go-to libraries for making beautiful plots and graphs. 📊
- **Joblib**: Helps us save and load machine learning models. 🔄
- **OS**: Lets us interact with the system (like making folders).

## **Step 2: Load the Dataset** 🗂️

Now, we’re going to load the dataset we’ll be working with. It's an **E-commerce Repeat Purchase Analysis** dataset, and it contains information about customer orders.

```python
df = pd.read_csv("../d2.ECommerce_Repeat_Purchase_Analysis.csv")
print("✅ Dataset Loaded")
```

### 📌 **Quick Dataset Overview:**

Looking at the dataset, I noticed a few interesting things:
- **Order Date**: The date of the purchase.
- **Customer ID**: Unique identifiers for customers.
- **Product Category**: What kind of product was purchased.
- **Customer Satisfaction Score**: How satisfied the customer was with their purchase.
- **Delivery Time**: How many days it took for the order to arrive.
- **Order Frequency**: The target variable, which is how often a customer makes purchases.

Let’s check out the first few rows to get an idea of what we're working with.

### **Output: Sample Rows** 👇

```plaintext
             Order Date   Customer ID   Product Category   Customer Satisfaction Score   Delivery Time (in Days)   Order Frequency
0  2023-11-26 21:37:00   CUST9201      Home Appliances          2.0                      14.0                     2
1  2022-12-24 00:26:00   CUST7209      Groceries                 3.0                      12.0                     8
2  2023-05-04 02:08:00   CUST3236      Beauty                    1.0                       2.0                     7
3  NaN                    CUST6180      Groceries                 4.0                      NaN                      10
4  2023-08-30 18:09:00   CUST1406      NaN                        3.0                       4.0                      10
```

### **Initial Observations** 🧐

- There are some **missing values** in **Order Date** and **Product Category** — we'll handle those soon.
- **Order Frequency** is our target variable, which represents how often a customer places an order.

## **Step 3: Basic Data Inspection** 🔍

Let’s now dive into the **structure** of the dataset. We’ll check:
- **Data types** 🛠️
- **Missing values** ❓
- **Summary statistics** 📊

```python
print("📌 Dataset Shape:", df.shape)
print("\n🔍 First 5 Rows:\n", df.head())
print("\n🧠 Info:")
df.info()
print("\n❓ Missing Values:\n", df.isnull().sum())
print("\n📊 Summary Stats:\n", df.describe())
```

### **Output: Summary Stats** 📊

```plaintext
        Customer Satisfaction Score  Delivery Time (in Days)  Order Frequency
count                   978.000000               979.000000      1030.000000
mean                      3.087935                 7.975485         5.302913
std                       1.423749                 4.277373         2.573977
min                       1.000000                 1.000000         1.000000
25%                       2.000000                 4.000000         3.000000
50%                       3.000000                 8.000000         5.000000
75%                       4.000000                12.000000         7.000000
max                       5.000000                15.000000        10.000000
```

### **Key Observations** 🔑:
- **Order Frequency** ranges from 1 to 10. Some customers buy a lot, while others are occasional shoppers.
- The **Delivery Time** ranges from 1 to 15 days. Looks like delivery times could have an impact on repeat purchases!

## **Step 4: Visualizations** 📈

Now, let’s visualize some key insights! We’ll start by plotting the **distribution** of the **Order Frequency** — our target variable.

```python
plt.figure(figsize=(6, 4))
sns.histplot(df['Order Frequency'], kde=True)
plt.title("Target Distribution: Order Frequency")
plt.show()
```

### **Expected Output: Histogram** 📊
![2 plot1](https://github.com/user-attachments/assets/b607a27a-d8d4-4edf-9b1e-d1181d5eff97)

This histogram shows us how often customers are placing orders. It’ll help us spot if there are any **skewed patterns** or if it’s roughly balanced.

## **Step 5: Feature Engineering with Dates** 📅

The **Order Date** column is in string format, but we can extract **new features** from it like:
- **Day of the month**
- **Month**
- **Year**
- **Weekday**
- **Hour of the day**

Let’s break it down and add these new features:

```python
df['Order Date'] = pd.to_datetime(df['Order Date'])
df['Order Day'] = df['Order Date'].dt.day
df['Order Month'] = df['Order Date'].dt.month
df['Order Year'] = df['Order Date'].dt.year
df['Order Weekday'] = df['Order Date'].dt.weekday
df['Order Hour'] = df['Order Date'].dt.hour
df.drop('Order Date', axis=1, inplace=True)
```

These new features can be helpful because customers might shop more during certain times of the day or specific months. 🎯

## **Step 6: Feature Correlation** 🔗

Now let’s see how the **features** relate to our **target variable** (Order Frequency). Correlation helps us identify which features are **more important** for prediction.

```python
numeric_df = df.select_dtypes(include=['number'])
correlation_with_target = numeric_df.corr()['Order Frequency'].drop('Order Frequency').sort_values(ascending=False)

plt.figure(figsize=(8, 5))
sns.barplot(x=correlation_with_target.values, y=correlation_with_target.index, palette='viridis')
plt.title("Feature Correlation with Target (Order Frequency)")
plt.xlabel("Correlation Coefficient")
plt.ylabel("Features")
plt.tight_layout()
plt.show()
```

### **Output: Feature Correlation Plot** 📊
![2 plot3](https://github.com/user-attachments/assets/599c43aa-ab76-419c-8160-eb4512144262)

From this plot, we’ll see which features are **most correlated** with our target. You might notice that **Customer Satisfaction** is strongly correlated with how often customers buy again!

### **Key Observations** 📌:
- **Customer Satisfaction** has a strong positive correlation with **Order Frequency**. Happy customers tend to purchase more frequently!
- **Delivery Time** has a negative correlation — customers tend to purchase more when deliveries are quicker. ⏱️



## Step 7: Feature Selection — *You're in Control!*

Now that we've engineered some cool features, it's time to zoom in on the **most important ones** — but here's the twist: **you get to decide** how many!

### 👉 Why Feature Selection?
Feature selection helps:
- Remove irrelevant/noisy data
- Reduce overfitting
- Speed up training
- Improve model performance 🚀

![Feature Selection](https://github.com/user-attachments/assets/e7c17efa-8969-4812-93e5-1ac310d9a652)

> **Nice!** These are the features that statistically have the strongest relationship with the subscription fee tier. This helps reduce noise and improve model performance!


## **Step 8: Feature Identification** 🏷️

Let’s separate our features into **numerical** and **categorical** columns. This helps us decide which preprocessing steps to apply.

```python
target = 'Order Frequency'
features = df.drop(target, axis=1)

categorical_cols = features.select_dtypes(include=['object']).columns.tolist()
numerical_cols = features.select_dtypes(include=['int64', 'float64']).columns.tolist()

print("🔢 Numerical Features:", numerical_cols)
print("🔤 Categorical Features:", categorical_cols)
```

### **Output: Feature List** 📜

```plaintext
🔢 Numerical Features: ['Customer Satisfaction Score', 'Delivery Time (in Days)', 'Order Day', 'Order Month', 'Order Year', 'Order Weekday', 'Order Hour']
🔤 Categorical Features: ['Customer ID', 'Product Category']
```

## **Step 9: Train-Test Split** 🔀

Let’s split the dataset into **training** and **testing** sets (80% for training and 20% for testing). This is crucial for evaluating model performance.

```python
X = df.drop(target, axis=1)
y = df[target]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

## **Step 10: Preprocessing Pipelines** 🔧

Next, we’ll set up preprocessing for both **numerical** and **categorical** data:
- **Numerical**: Impute missing values and scale features.
- **Categorical**: Impute missing values and one-hot encode.

```python
numeric_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())])
categorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')), ('encoder', OneHotEncoder(handle_unknown='ignore'))])

preprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, numerical_cols), ('cat', categorical_transformer, categorical_cols)])
```
## 🎯 Step 11: Model Selection & Training

Alright, it’s time to bring in the machine learning models! Since our target variable — `Order Frequency` — is a continuous numeric value, we’re dealing with a **regression problem**.

But before jumping into training, let’s talk about our model choices. There are lots of algorithms we *could* use, but not all of them are equally suited for every dataset. So we’ll start by listing a few strong contenders, and then we’ll narrow it down to a smaller set of models we’ll actually train.

---

### 🔍 Potential Models for Regression

Here are 7–8 models that can work well for this type of regression task:

1. **Linear Regression**  
   The simplest and most interpretable model. It assumes a linear relationship between features and the target. Great for benchmarking and understanding basic trends.

2. **Ridge & Lasso Regression**  
   These are regularized versions of Linear Regression:
   - **Ridge** helps when we have multicollinearity.
   - **Lasso** also performs feature selection by shrinking less important feature weights to zero.

3. **Decision Tree Regressor** 🌳  
   A non-linear model that splits the data based on feature values. Very interpretable and can capture complex relationships, but prone to overfitting if not tuned properly.

4. **Random Forest Regressor** 🌲🌲  
   An ensemble of decision trees that averages their results. It’s robust, reduces overfitting, and handles missing values & non-linearities quite well.

5. **Gradient Boosting Regressor (GBR)** 🚀  
   Builds trees sequentially, each learning from the errors of the last. Usually outperforms Random Forests in terms of accuracy when tuned correctly.

6. **XGBoost Regressor**  
   An optimized and regularized version of Gradient Boosting. It’s fast, powerful, and often wins competitions (like Kaggle!).

7. **Support Vector Regressor (SVR)**  
   Useful for datasets where you want to control margins and handle outliers well. But it doesn’t scale great with large datasets and requires careful preprocessing.

8. **K-Nearest Neighbors Regressor (KNN)**  
   A lazy learner that predicts based on the average of the `k` closest points. Simple and intuitive, but can struggle with high-dimensional data.

---

![Feature Selection](https://github.com/user-attachments/assets/e7c17efa-8969-4812-93e5-1ac310d9a652)

### ✅ Selected Models for Our Analysis

To keep things efficient and meaningful, we’ll focus on the following 4–5 models:

| Model | Why We Chose It |
|-------|------------------|
| **Linear Regression** | It’s always good to start with a simple baseline. Quick to train and easy to interpret. |
| **Ridge Regression** | A great regularized model that improves on basic linear regression when multicollinearity is present. |
| **Random Forest Regressor** | Handles feature interactions and non-linearities well. Very reliable out-of-the-box performer. |
| **Gradient Boosting Regressor (GBR)** | A powerful model that tends to outperform others when properly tuned. Great for handling messy data. |
| **XGBoost Regressor** | An advanced version of GBR that’s fast and often gives state-of-the-art results. |

These models give us a mix of **simplicity**, **interpretability**, and **power**. We’ll train each, evaluate performance on the test set, and then pick the best one for saving and deployment. 🚀

Let’s get modeling! 🧠👇


Absolutely! Here's a continuation in the same tone, customized for your **E-commerce Repeat Purchase Analysis** regression task:

---

### 🧠 Step 12: Conclusion & Next Steps 
We’re now ready to train our selected regression models and evaluate how well they can **predict how often a customer places an order** — the key metric for our analysis.

We’ll assess each model’s performance using **regression metrics** like:

- **RMSE (Root Mean Squared Error)**: Measures the average prediction error magnitude.
- **MAE (Mean Absolute Error)**: Gives a clearer, easy-to-interpret error in original units.
- **R² Score**: Tells us how much variance in the target is explained by the model.

📉 These metrics will help us understand how close our predicted order frequency is to the actual behavior.

🔄 If the initial results aren’t satisfactory, don’t worry — you’ll have the option to fine-tune the models or experiment with others from our broader pool.
- ✅ **Make predictions** on unseen data using base models
- 🎛️ **Fine-tune hyperparameters** using grid search or randomized search.
- ✅ **Make predictions on tuned mdoels** on unseen data 

---

### 💬 Final Thoughts:

This next phase is where **data turns into value**. By comparing model performances, we can identify the algorithm that most effectively predicts customer behavior — a powerful tool for **increasing repeat purchases** and **boosting revenue**.

You’ve laid the foundation, engineered smart features, and narrowed down your model choices — now it’s time to let the models shine. ✨

Let’s jump into model generation and see which one comes out on top! 🏆


