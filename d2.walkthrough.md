
---

# 🎉 **Welcome to the Data Science Walkthrough!** 🎉

Hey there! 😊 Welcome to this interactive **Jupyter Notebook** session! We're going to explore a dataset, perform some cool data wrangling, and build models together. Think of this as a fun, hands-on data science tutorial where I'll guide you step by step. 💡

## **Step 1: Import Libraries** 📚

Let's kick things off by importing the libraries we need for this project. These libraries will help us load, preprocess, and model the data, plus some tools for visualizing our results.

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import os
```

- **Pandas**: Manages the data tables.
- **NumPy**: Does the math and heavy lifting.
- **Matplotlib & Seaborn**: These are our go-to libraries for making beautiful plots and graphs. 📊
- **Joblib**: Helps us save and load machine learning models. 🔄
- **OS**: Lets us interact with the system (like making folders).

## **Step 2: Load the Dataset** 🗂️

Now, we’re going to load the dataset we’ll be working with. It's an **E-commerce Repeat Purchase Analysis** dataset, and it contains information about customer orders.

```python
df = pd.read_csv("../d2.ECommerce_Repeat_Purchase_Analysis.csv")
print("✅ Dataset Loaded")
```

### 📌 **Quick Dataset Overview:**

Looking at the dataset, I noticed a few interesting things:
- **Order Date**: The date of the purchase.
- **Customer ID**: Unique identifiers for customers.
- **Product Category**: What kind of product was purchased.
- **Customer Satisfaction Score**: How satisfied the customer was with their purchase.
- **Delivery Time**: How many days it took for the order to arrive.
- **Order Frequency**: The target variable, which is how often a customer makes purchases.

Let’s check out the first few rows to get an idea of what we're working with.

### **Output: Sample Rows** 👇

```plaintext
             Order Date   Customer ID   Product Category   Customer Satisfaction Score   Delivery Time (in Days)   Order Frequency
0  2023-11-26 21:37:00   CUST9201      Home Appliances          2.0                      14.0                     2
1  2022-12-24 00:26:00   CUST7209      Groceries                 3.0                      12.0                     8
2  2023-05-04 02:08:00   CUST3236      Beauty                    1.0                       2.0                     7
3  NaN                    CUST6180      Groceries                 4.0                      NaN                      10
4  2023-08-30 18:09:00   CUST1406      NaN                        3.0                       4.0                      10
```

### **Initial Observations** 🧐

- There are some **missing values** in **Order Date** and **Product Category** — we'll handle those soon.
- **Order Frequency** is our target variable, which represents how often a customer places an order.

## **Step 3: Basic Data Inspection** 🔍

Let’s now dive into the **structure** of the dataset. We’ll check:
- **Data types** 🛠️
- **Missing values** ❓
- **Summary statistics** 📊

```python
print("📌 Dataset Shape:", df.shape)
print("\n🔍 First 5 Rows:\n", df.head())
print("\n🧠 Info:")
df.info()
print("\n❓ Missing Values:\n", df.isnull().sum())
print("\n📊 Summary Stats:\n", df.describe())
```

### **Output: Summary Stats** 📊

```plaintext
        Customer Satisfaction Score  Delivery Time (in Days)  Order Frequency
count                   978.000000               979.000000      1030.000000
mean                      3.087935                 7.975485         5.302913
std                       1.423749                 4.277373         2.573977
min                       1.000000                 1.000000         1.000000
25%                       2.000000                 4.000000         3.000000
50%                       3.000000                 8.000000         5.000000
75%                       4.000000                12.000000         7.000000
max                       5.000000                15.000000        10.000000
```

### **Key Observations** 🔑:
- **Order Frequency** ranges from 1 to 10. Some customers buy a lot, while others are occasional shoppers.
- The **Delivery Time** ranges from 1 to 15 days. Looks like delivery times could have an impact on repeat purchases!

## **Step 4: Visualizations** 📈

Now, let’s visualize some key insights! We’ll start by plotting the **distribution** of the **Order Frequency** — our target variable.

```python
plt.figure(figsize=(6, 4))
sns.histplot(df['Order Frequency'], kde=True)
plt.title("Target Distribution: Order Frequency")
plt.show()
```

### **Expected Output: Histogram** 📊

This histogram shows us how often customers are placing orders. It’ll help us spot if there are any **skewed patterns** or if it’s roughly balanced.

## **Step 5: Feature Engineering with Dates** 📅

The **Order Date** column is in string format, but we can extract **new features** from it like:
- **Day of the month**
- **Month**
- **Year**
- **Weekday**
- **Hour of the day**

Let’s break it down and add these new features:

```python
df['Order Date'] = pd.to_datetime(df['Order Date'])
df['Order Day'] = df['Order Date'].dt.day
df['Order Month'] = df['Order Date'].dt.month
df['Order Year'] = df['Order Date'].dt.year
df['Order Weekday'] = df['Order Date'].dt.weekday
df['Order Hour'] = df['Order Date'].dt.hour
df.drop('Order Date', axis=1, inplace=True)
```

These new features can be helpful because customers might shop more during certain times of the day or specific months. 🎯

## **Step 6: Feature Correlation** 🔗

Now let’s see how the **features** relate to our **target variable** (Order Frequency). Correlation helps us identify which features are **more important** for prediction.

```python
numeric_df = df.select_dtypes(include=['number'])
correlation_with_target = numeric_df.corr()['Order Frequency'].drop('Order Frequency').sort_values(ascending=False)

plt.figure(figsize=(8, 5))
sns.barplot(x=correlation_with_target.values, y=correlation_with_target.index, palette='viridis')
plt.title("Feature Correlation with Target (Order Frequency)")
plt.xlabel("Correlation Coefficient")
plt.ylabel("Features")
plt.tight_layout()
plt.show()
```

### **Output: Feature Correlation Plot** 📊

From this plot, we’ll see which features are **most correlated** with our target. You might notice that **Customer Satisfaction** is strongly correlated with how often customers buy again!

### **Key Observations** 📌:
- **Customer Satisfaction** has a strong positive correlation with **Order Frequency**. Happy customers tend to purchase more frequently!
- **Delivery Time** has a negative correlation — customers tend to purchase more when deliveries are quicker. ⏱️

## **Step 7: Feature Identification** 🏷️

Let’s separate our features into **numerical** and **categorical** columns. This helps us decide which preprocessing steps to apply.

```python
target = 'Order Frequency'
features = df.drop(target, axis=1)

categorical_cols = features.select_dtypes(include=['object']).columns.tolist()
numerical_cols = features.select_dtypes(include=['int64', 'float64']).columns.tolist()

print("🔢 Numerical Features:", numerical_cols)
print("🔤 Categorical Features:", categorical_cols)
```

### **Output: Feature List** 📜

```plaintext
🔢 Numerical Features: ['Customer Satisfaction Score', 'Delivery Time (in Days)', 'Order Day', 'Order Month', 'Order Year', 'Order Weekday', 'Order Hour']
🔤 Categorical Features: ['Customer ID', 'Product Category']
```

## **Step 8: Train-Test Split** 🔀

Let’s split the dataset into **training** and **testing** sets (80% for training and 20% for testing). This is crucial for evaluating model performance.

```python
X = df.drop(target, axis=1)
y = df[target]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

## **Step 9: Preprocessing Pipelines** 🔧

Next, we’ll set up preprocessing for both **numerical** and **categorical** data:
- **Numerical**: Impute missing values and scale features.
- **Categorical**: Impute missing values and one-hot encode.

```python
numeric_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())])
categorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')), ('encoder', OneHotEncoder(handle_unknown='ignore'))])

preprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, numerical_cols), ('cat', categorical_transformer, categorical_cols)])
```

## **Step 10: Model Training** 🧑‍💻

Let's train multiple models: **Ridge**, **Random Forest**, **Gradient Boosting**, and **SVR**. We’ll save the trained models using **joblib** for later use.

```python
models = {
    'Ridge': Ridge(),
    'Random Forest': RandomForestRegressor(),
    'Gradient Boosting': GradientBoostingRegressor(),
    'SVR': SVR()
}

for model_name, model in models.items():
    print(f"Training {model_name}...")
    model_pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('model', model)])
    model_pipeline.fit(X_train, y_train)
    joblib.dump(model_pipeline, f'{model_name}_model.pkl')
    print(f"✅ {model_name} Model Trained and Saved")
```

### **Conclusion** 🏁

Well done! 🎉 You've just preprocessed the dataset, explored the features, and trained multiple models. We can now move on to evaluating the models or even fine-tuning them.

If you're ready, just let me know what you'd like to explore next, whether it's **model evaluation** or **hyperparameter tuning**. 👨‍💻
