
---

# ğŸ‰ **Welcome to the Data Science Walkthrough!** ğŸ‰

Hey there! ğŸ˜Š Welcome to this interactive **Jupyter Notebook** session! We're going to explore a dataset, perform some cool data wrangling, and build models together. Think of this as a fun, hands-on data science tutorial where I'll guide you step by step. ğŸ’¡

## **Step 1: Import Libraries** ğŸ“š

Let's kick things off by importing the libraries we need for this project. These libraries will help us load, preprocess, and model the data, plus some tools for visualizing our results.

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import os
```

- **Pandas**: Manages the data tables.
- **NumPy**: Does the math and heavy lifting.
- **Matplotlib & Seaborn**: These are our go-to libraries for making beautiful plots and graphs. ğŸ“Š
- **Joblib**: Helps us save and load machine learning models. ğŸ”„
- **OS**: Lets us interact with the system (like making folders).

## **Step 2: Load the Dataset** ğŸ—‚ï¸

Now, weâ€™re going to load the dataset weâ€™ll be working with. It's an **E-commerce Repeat Purchase Analysis** dataset, and it contains information about customer orders.

```python
df = pd.read_csv("../d2.ECommerce_Repeat_Purchase_Analysis.csv")
print("âœ… Dataset Loaded")
```

### ğŸ“Œ **Quick Dataset Overview:**

Looking at the dataset, I noticed a few interesting things:
- **Order Date**: The date of the purchase.
- **Customer ID**: Unique identifiers for customers.
- **Product Category**: What kind of product was purchased.
- **Customer Satisfaction Score**: How satisfied the customer was with their purchase.
- **Delivery Time**: How many days it took for the order to arrive.
- **Order Frequency**: The target variable, which is how often a customer makes purchases.

Letâ€™s check out the first few rows to get an idea of what we're working with.

### **Output: Sample Rows** ğŸ‘‡

```plaintext
             Order Date   Customer ID   Product Category   Customer Satisfaction Score   Delivery Time (in Days)   Order Frequency
0  2023-11-26 21:37:00   CUST9201      Home Appliances          2.0                      14.0                     2
1  2022-12-24 00:26:00   CUST7209      Groceries                 3.0                      12.0                     8
2  2023-05-04 02:08:00   CUST3236      Beauty                    1.0                       2.0                     7
3  NaN                    CUST6180      Groceries                 4.0                      NaN                      10
4  2023-08-30 18:09:00   CUST1406      NaN                        3.0                       4.0                      10
```

### **Initial Observations** ğŸ§

- There are some **missing values** in **Order Date** and **Product Category** â€” we'll handle those soon.
- **Order Frequency** is our target variable, which represents how often a customer places an order.

## **Step 3: Basic Data Inspection** ğŸ”

Letâ€™s now dive into the **structure** of the dataset. Weâ€™ll check:
- **Data types** ğŸ› ï¸
- **Missing values** â“
- **Summary statistics** ğŸ“Š

```python
print("ğŸ“Œ Dataset Shape:", df.shape)
print("\nğŸ” First 5 Rows:\n", df.head())
print("\nğŸ§  Info:")
df.info()
print("\nâ“ Missing Values:\n", df.isnull().sum())
print("\nğŸ“Š Summary Stats:\n", df.describe())
```

### **Output: Summary Stats** ğŸ“Š

```plaintext
        Customer Satisfaction Score  Delivery Time (in Days)  Order Frequency
count                   978.000000               979.000000      1030.000000
mean                      3.087935                 7.975485         5.302913
std                       1.423749                 4.277373         2.573977
min                       1.000000                 1.000000         1.000000
25%                       2.000000                 4.000000         3.000000
50%                       3.000000                 8.000000         5.000000
75%                       4.000000                12.000000         7.000000
max                       5.000000                15.000000        10.000000
```

### **Key Observations** ğŸ”‘:
- **Order Frequency** ranges from 1 to 10. Some customers buy a lot, while others are occasional shoppers.
- The **Delivery Time** ranges from 1 to 15 days. Looks like delivery times could have an impact on repeat purchases!

## **Step 4: Visualizations** ğŸ“ˆ

Now, letâ€™s visualize some key insights! Weâ€™ll start by plotting the **distribution** of the **Order Frequency** â€” our target variable.

```python
plt.figure(figsize=(6, 4))
sns.histplot(df['Order Frequency'], kde=True)
plt.title("Target Distribution: Order Frequency")
plt.show()
```

### **Expected Output: Histogram** ğŸ“Š
![2 plot1](https://github.com/user-attachments/assets/b607a27a-d8d4-4edf-9b1e-d1181d5eff97)

This histogram shows us how often customers are placing orders. Itâ€™ll help us spot if there are any **skewed patterns** or if itâ€™s roughly balanced.

## **Step 5: Feature Engineering with Dates** ğŸ“…

The **Order Date** column is in string format, but we can extract **new features** from it like:
- **Day of the month**
- **Month**
- **Year**
- **Weekday**
- **Hour of the day**

Letâ€™s break it down and add these new features:

```python
df['Order Date'] = pd.to_datetime(df['Order Date'])
df['Order Day'] = df['Order Date'].dt.day
df['Order Month'] = df['Order Date'].dt.month
df['Order Year'] = df['Order Date'].dt.year
df['Order Weekday'] = df['Order Date'].dt.weekday
df['Order Hour'] = df['Order Date'].dt.hour
df.drop('Order Date', axis=1, inplace=True)
```

These new features can be helpful because customers might shop more during certain times of the day or specific months. ğŸ¯

## **Step 6: Feature Correlation** ğŸ”—

Now letâ€™s see how the **features** relate to our **target variable** (Order Frequency). Correlation helps us identify which features are **more important** for prediction.

```python
numeric_df = df.select_dtypes(include=['number'])
correlation_with_target = numeric_df.corr()['Order Frequency'].drop('Order Frequency').sort_values(ascending=False)

plt.figure(figsize=(8, 5))
sns.barplot(x=correlation_with_target.values, y=correlation_with_target.index, palette='viridis')
plt.title("Feature Correlation with Target (Order Frequency)")
plt.xlabel("Correlation Coefficient")
plt.ylabel("Features")
plt.tight_layout()
plt.show()
```

### **Output: Feature Correlation Plot** ğŸ“Š
![2 plot3](https://github.com/user-attachments/assets/599c43aa-ab76-419c-8160-eb4512144262)

From this plot, weâ€™ll see which features are **most correlated** with our target. You might notice that **Customer Satisfaction** is strongly correlated with how often customers buy again!

### **Key Observations** ğŸ“Œ:
- **Customer Satisfaction** has a strong positive correlation with **Order Frequency**. Happy customers tend to purchase more frequently!
- **Delivery Time** has a negative correlation â€” customers tend to purchase more when deliveries are quicker. â±ï¸



## Step 7: Feature Selection â€” *You're in Control!*

Now that we've engineered some cool features, it's time to zoom in on the **most important ones** â€” but here's the twist: **you get to decide** how many!

### ğŸ‘‰ Why Feature Selection?
Feature selection helps:
- Remove irrelevant/noisy data
- Reduce overfitting
- Speed up training
- Improve model performance ğŸš€

![Feature Selection](https://github.com/user-attachments/assets/e7c17efa-8969-4812-93e5-1ac310d9a652)

> **Nice!** These are the features that statistically have the strongest relationship with the subscription fee tier. This helps reduce noise and improve model performance!


## **Step 8: Feature Identification** ğŸ·ï¸

Letâ€™s separate our features into **numerical** and **categorical** columns. This helps us decide which preprocessing steps to apply.

```python
target = 'Order Frequency'
features = df.drop(target, axis=1)

categorical_cols = features.select_dtypes(include=['object']).columns.tolist()
numerical_cols = features.select_dtypes(include=['int64', 'float64']).columns.tolist()

print("ğŸ”¢ Numerical Features:", numerical_cols)
print("ğŸ”¤ Categorical Features:", categorical_cols)
```

### **Output: Feature List** ğŸ“œ

```plaintext
ğŸ”¢ Numerical Features: ['Customer Satisfaction Score', 'Delivery Time (in Days)', 'Order Day', 'Order Month', 'Order Year', 'Order Weekday', 'Order Hour']
ğŸ”¤ Categorical Features: ['Customer ID', 'Product Category']
```

## **Step 9: Train-Test Split** ğŸ”€

Letâ€™s split the dataset into **training** and **testing** sets (80% for training and 20% for testing). This is crucial for evaluating model performance.

```python
X = df.drop(target, axis=1)
y = df[target]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

## **Step 10: Preprocessing Pipelines** ğŸ”§

Next, weâ€™ll set up preprocessing for both **numerical** and **categorical** data:
- **Numerical**: Impute missing values and scale features.
- **Categorical**: Impute missing values and one-hot encode.

```python
numeric_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())])
categorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')), ('encoder', OneHotEncoder(handle_unknown='ignore'))])

preprocessor = ColumnTransformer(transformers=[('num', numeric_transformer, numerical_cols), ('cat', categorical_transformer, categorical_cols)])
```
## ğŸ¯ Step 11: Model Selection & Training

Alright, itâ€™s time to bring in the machine learning models! Since our target variable â€” `Order Frequency` â€” is a continuous numeric value, weâ€™re dealing with a **regression problem**.

But before jumping into training, letâ€™s talk about our model choices. There are lots of algorithms we *could* use, but not all of them are equally suited for every dataset. So weâ€™ll start by listing a few strong contenders, and then weâ€™ll narrow it down to a smaller set of models weâ€™ll actually train.

---

### ğŸ” Potential Models for Regression

Here are 7â€“8 models that can work well for this type of regression task:

1. **Linear Regression**  
   The simplest and most interpretable model. It assumes a linear relationship between features and the target. Great for benchmarking and understanding basic trends.

2. **Ridge & Lasso Regression**  
   These are regularized versions of Linear Regression:
   - **Ridge** helps when we have multicollinearity.
   - **Lasso** also performs feature selection by shrinking less important feature weights to zero.

3. **Decision Tree Regressor** ğŸŒ³  
   A non-linear model that splits the data based on feature values. Very interpretable and can capture complex relationships, but prone to overfitting if not tuned properly.

4. **Random Forest Regressor** ğŸŒ²ğŸŒ²  
   An ensemble of decision trees that averages their results. Itâ€™s robust, reduces overfitting, and handles missing values & non-linearities quite well.

5. **Gradient Boosting Regressor (GBR)** ğŸš€  
   Builds trees sequentially, each learning from the errors of the last. Usually outperforms Random Forests in terms of accuracy when tuned correctly.

6. **XGBoost Regressor**  
   An optimized and regularized version of Gradient Boosting. Itâ€™s fast, powerful, and often wins competitions (like Kaggle!).

7. **Support Vector Regressor (SVR)**  
   Useful for datasets where you want to control margins and handle outliers well. But it doesnâ€™t scale great with large datasets and requires careful preprocessing.

8. **K-Nearest Neighbors Regressor (KNN)**  
   A lazy learner that predicts based on the average of the `k` closest points. Simple and intuitive, but can struggle with high-dimensional data.

---

![Feature Selection](https://github.com/user-attachments/assets/e7c17efa-8969-4812-93e5-1ac310d9a652)

### âœ… Selected Models for Our Analysis

To keep things efficient and meaningful, weâ€™ll focus on the following 4â€“5 models:

| Model | Why We Chose It |
|-------|------------------|
| **Linear Regression** | Itâ€™s always good to start with a simple baseline. Quick to train and easy to interpret. |
| **Ridge Regression** | A great regularized model that improves on basic linear regression when multicollinearity is present. |
| **Random Forest Regressor** | Handles feature interactions and non-linearities well. Very reliable out-of-the-box performer. |
| **Gradient Boosting Regressor (GBR)** | A powerful model that tends to outperform others when properly tuned. Great for handling messy data. |
| **XGBoost Regressor** | An advanced version of GBR thatâ€™s fast and often gives state-of-the-art results. |

These models give us a mix of **simplicity**, **interpretability**, and **power**. Weâ€™ll train each, evaluate performance on the test set, and then pick the best one for saving and deployment. ğŸš€

Letâ€™s get modeling! ğŸ§ ğŸ‘‡


Absolutely! Here's a continuation in the same tone, customized for your **E-commerce Repeat Purchase Analysis** regression task:

---

### ğŸ§  Step 12: Conclusion & Next Steps 
Weâ€™re now ready to train our selected regression models and evaluate how well they can **predict how often a customer places an order** â€” the key metric for our analysis.

Weâ€™ll assess each modelâ€™s performance using **regression metrics** like:

- **RMSE (Root Mean Squared Error)**: Measures the average prediction error magnitude.
- **MAE (Mean Absolute Error)**: Gives a clearer, easy-to-interpret error in original units.
- **RÂ² Score**: Tells us how much variance in the target is explained by the model.

ğŸ“‰ These metrics will help us understand how close our predicted order frequency is to the actual behavior.

ğŸ”„ If the initial results arenâ€™t satisfactory, donâ€™t worry â€” youâ€™ll have the option to fine-tune the models or experiment with others from our broader pool.
- âœ… **Make predictions** on unseen data using base models
- ğŸ›ï¸ **Fine-tune hyperparameters** using grid search or randomized search.
- âœ… **Make predictions on tuned mdoels** on unseen data 

---

### ğŸ’¬ Final Thoughts:

This next phase is where **data turns into value**. By comparing model performances, we can identify the algorithm that most effectively predicts customer behavior â€” a powerful tool for **increasing repeat purchases** and **boosting revenue**.

Youâ€™ve laid the foundation, engineered smart features, and narrowed down your model choices â€” now itâ€™s time to let the models shine. âœ¨

Letâ€™s jump into model generation and see which one comes out on top! ğŸ†


