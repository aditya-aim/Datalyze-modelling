# Welcome to the Walkthrough! ðŸš€

Hello and welcome! Today, we're going to work through an insurance claims dataset, analyze it, and build some predictive models to predict whether a claim will be approved or not. By the end of this walkthrough, you'll have learned how to explore data, engineer features, build multiple models, and evaluate them. I'll be guiding you step by step through each section, explaining what's happening, and how to make sense of the outputs.

## Step 1: Dataset Overview ðŸ“Š

Let's start by taking a look at the dataset we're working with. It's an insurance claims dataset that contains information about different claims, such as the type of policy, claim amounts, customer age group, and whether or not the claim was approved.

When we load the dataset, here's what we see:

```python
df = pd.read_csv("d6.Insurance_Claims_Analysis.csv")
print("âœ… Dataset Loaded.")
print(df.head())
```

Output:
```
âœ… Dataset Loaded.
   Claim Date Policy Type  Claim Amount  Approval Status Customer Age Group  Approval Time
0  2024-06-10      Health      52389.10                1              26-40               155
1  2024-06-07        Home      24479.91                1                41+               113
2  2024-09-04        Life       7179.58                1              18-25                57
3  2023-05-05       Auto       7154.33                1                41+               121
4  2025-05-20      Health      38639.37                0                41+               138
```

**Looking at the dataset, I noticed:**
- **Policy Type**: Categorical column representing different insurance policy types (Health, Home, Auto, Life)
- **Claim Amount**: Numerical data indicating the amount claimed
- **Approval Status**: Binary target variable (0 = Rejected, 1 = Approved)
- **Customer Age Group**: Categorical variable representing the customer's age group
- **Approval Time**: Numerical variable showing the time it took for the claim to be approved

## Step 2: Basic Data Exploration ðŸ”

Let's check out some basic info, descriptive statistics, and look for any missing values:

```python
print("\nðŸ“Š Dataset Info:")
print(df.info())

print("\nðŸ§¾ Descriptive Statistics:")
print(df.describe())

print("\nðŸ” Null Values:")
print(df.isnull().sum())
```

**Output:**

- **Dataset Info**: We have 1000 rows and 6 columns, and it appears there are no missing values in the dataset
- **Descriptive Statistics**: We see that the claim amounts have a wide range, from about $697 to $99,836. The approval status shows a majority of claims are approved (761 out of 1000)
- **Null Values**: There are no missing values in any of the columns, so we can proceed without handling missing data

## Step 3: Target Variable Distribution ðŸŽ¯

Let's take a closer look at how the target variable, **Approval Status**, is distributed.

```python
print("\nðŸŽ¯ Target Variable Distribution:")
print(df['Approval Status'].value_counts())

plt.figure(figsize=(6, 4))
sns.countplot(x='Approval Status', data=df, palette='Set2')
plt.title("Approval Status Distribution")
plt.xlabel("Approval Status")
plt.ylabel("Count")
plt.show()
```

**Output:**
![6 PLOT1](https://github.com/user-attachments/assets/32eaf674-6334-4d9a-b475-4309b06b3714)

```
Approval Status
1    761
0    239
Name: count, dtype: int64
```

**One interesting thing that stood out** is that the dataset is imbalanced, with more approved claims (1) than rejected claims (0). This could influence model performance, so we'll need to keep this in mind during modeling.

## Step 4: Categorical Feature Distributions ðŸ“Œ

Next, let's explore the distributions of categorical features, namely **Policy Type** and **Customer Age Group**.

```python
cat_features = ['Policy Type', 'Customer Age Group']
for col in cat_features:
    print(f"\nðŸ“Œ {col} Distribution:")
    print(df[col].value_counts())
    plt.figure(figsize=(6, 4))
    sns.countplot(x=col, data=df, palette='Pastel1')
    plt.title(f"{col} Distribution")
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()
```

**Output:**
![image (4)](https://github.com/user-attachments/assets/3d1796f4-bee7-4c37-a003-3d91fa60ed70)



- **Policy Type**: There are 421 Health claims, 199 Home, 192 Auto, and 188 Life claims
- **Customer Age Group**: The dataset is fairly evenly distributed across age groups (0-17, 18-25, 26-40, and 41+)

## Step 5: Numerical Feature Distributions ðŸ’¸

Now, let's explore the **Claim Amount** column, a numerical feature that we'll scale later.

```python
num_features = ['Claim Amount']
for col in num_features:
    print(f"\nðŸ’¸ {col} Statistics:")
    print(df[col].describe())
    plt.figure(figsize=(6, 4))
    sns.histplot(df[col], bins=30, kde=True, color='skyblue')
    plt.title(f"{col} Distribution")
    plt.show()
```

**Output:**
![6 PLOT4](https://github.com/user-attachments/assets/caae3f07-7220-4ce0-a641-798b59bd5122)

- The **Claim Amount** ranges from $697 to $99,836, with a mean around $37,307. This indicates that the dataset has a right-skewed distribution, which is quite common for financial data

## Step 6: Feature Engineering ðŸ“…

Let's create a new feature, **Days Since First Claim**, to capture the time difference between each claim and the first claim in the dataset.

```python
df['Claim Date'] = pd.to_datetime(df['Claim Date'])
df['Days Since First Claim'] = (df['Claim Date'] - df['Claim Date'].min()).dt.days
```

**Output:**
- **Date Range**: We now know that the dataset spans from 2023 to 2025

**One interesting thing to note** is that we can track the number of claims over time and visualize the trend.

```python
claims_per_day = df['Claim Date'].value_counts().sort_index()
claims_per_day.head()

plt.figure(figsize=(10, 4))
claims_per_day.plot()
plt.title("Claims Over Time")
plt.ylabel("Number of Claims")
plt.xlabel("Claim Date")
plt.show()
```
![6 PLOT5](https://github.com/user-attachments/assets/4931055b-ade2-43a4-9cae-942ce9ba9dc1)

This will give us a time-series plot showing how claims have been distributed over time.

## Step 7: Correlation Matrix ðŸ“Š

Let's analyze the correlation between numerical features to see how they relate.

```python
corr_features = ['Claim Amount', 'Days Since First Claim']
corr_matrix = df[corr_features].corr()
sns.heatmap(corr_matrix, annot=True, cmap='Blues')
plt.title("Correlation Matrix")
plt.show()
```

**Output:**
- There's a very weak negative correlation (-0.02) between **Claim Amount** and **Days Since First Claim**, which isn't surprising since they likely don't have a direct relationship

## Step 8: Encoding Categorical Variables ðŸ§¬

Since most machine learning models require numerical data, we'll encode the categorical variables.

```python
label_encoders = {}
for col in cat_features:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le
```

**Output:**
- **Policy Type** and **Customer Age Group** are now encoded as numeric values:
  - **Policy Type**: Auto = 0, Health = 1, Home = 2, Life = 3
  - **Customer Age Group**: 0-17 = 0, 18-25 = 1, 26-40 = 2, 41+ = 3

## Step 9: Feature Importance for Target ðŸ“ˆ

Now that we've prepared and encoded our data, it's time to look at **how each feature correlates with the target** variable, which in this case, is the **"Approval Status"**. This is important because it helps us understand which features are most influential when it comes to predicting approval status. Features with high correlations will likely be more useful for predictive models, so let's take a look at that!

### Code:

```python
# Calculate correlation of all features with the target
target_corr = df.corr(numeric_only=True)['Approval Status'].drop('Approval Status').sort_values()

# Plot
plt.figure(figsize=(8,5))
sns.barplot(x=target_corr.values, y=target_corr.index, palette="coolwarm")

# Styling
plt.title("Feature Correlation with Target (Approval Status)", fontsize=14)
plt.xlabel("Correlation Coefficient")
plt.ylabel("Features")
plt.axvline(0, color='black', linewidth=0.8, linestyle='--')
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()
```
![6 PLOT6](https://github.com/user-attachments/assets/0da44176-7042-430b-8d69-7209f38eb90b)

### What we see from the output:

- **Features with strong correlations:**
  - **Feature A** might have a high positive or negative correlation with the target, suggesting it has a major impact on whether something gets approved or not
  
- **Features with weak correlations:**
  - **Feature B** might show little to no correlation, meaning it doesn't influence approval status much

## Step 10: Feature Selection ðŸ“Š

Now that we've engineered some cool features, it's time to zoom in on the **most important ones** â€” but here's the twist: **you get to decide** how many!

### Why Feature Selection?
Feature selection helps:
- Remove irrelevant/noisy data
- Reduce overfitting
- Speed up training
- Improve model performance ðŸš€


![Feature Selection](https://github.com/user-attachments/assets/e7c17efa-8969-4812-93e5-1ac310d9a652)

> **Nice!** These are the features that statistically have the strongest relationship with the target. This helps reduce noise and improve model performance!


## Step 11: Train-Test Split ðŸ“

Before we start modeling, we'll split the data into training and test sets.

```python
X = df[features]
y = df[target]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
```

**Output:**
- We now have 800 rows for training and 200 for testing

## Step 12: Scaling the Features ðŸ“

Next, let's scale the numerical features to make sure that models like logistic regression and SVM perform well.

```python
scaler = StandardScaler()
X_train[num_cols] = scaler.fit_transform(X_train[num_cols])
X_test[num_cols] = scaler.transform(X_test[num_cols])
```

## Step 13: Predictive Modeling ðŸ—ï¸

Now that we've completed data cleaning, exploration, and feature engineering, it's time to move into **predictive modeling** â€” our goal:  
ðŸŽ¯ **Predict the Subscription Fee** for each user based on the features we've engineered and explored.

### Modeling Strategy

We explored multiple modeling approaches â€” both **regression and classification** â€” to capture a variety of data patterns. While the primary goal is to predict the **subscription fee (a continuous value)**, we also considered **customer behavior prediction** (like upgrade/downgrade) to support business decisions.

### Initial Models Considered

These models were evaluated during experimentation:

- **Random Forest Regressor** â€“ Ensemble tree-based model, good for complex data
- **Gradient Boosting Regressor** â€“ Boosted trees that improve performance iteratively
- **Logistic Regression** â€“ Linear model, used as a baseline
- **SVM Regressor** â€“ Kernel-based model, good for high-dimensional and non-linear relationships
- **K-Nearest Neighbors** â€“ Captures local similarity patterns
- **Linear Regression** â€“ Basic linear baseline
- **XGBoost Regressor** â€“ Advanced boosting model, handles missing values well
- **Subscription Change Predictor (Classification)** â€“ Predicts customer behavior (upgrade/downgrade)

![Feature Selection](https://github.com/user-attachments/assets/e7c17efa-8969-4812-93e5-1ac310d9a652)

### Final Models Selected

After experimenting and analyzing feature relationships, we selected the **most relevant models** based on the dataset's characteristics:

```python
model_names = ['LogisticRegression_model', 'RandomForest_model', 'SVM_model', 'XGBoost_model']
```

Here's **why we chose them**:

- **ðŸ“ˆ Logistic Regression**  
  *Why?* A simple and interpretable model, great as a baseline. It helps reveal any linear patterns in the data and provides easily explainable feature impacts.

- **ðŸŒ³ Random Forest Regressor**  
  *Why?* A robust ensemble method that handles both categorical and numerical data well. It's resistant to overfitting and excels at capturing complex feature interactions.

- **ðŸ” Support Vector Machine (SVM) Regressor**  
  *Why?* Highly effective in high-dimensional spaces, SVM captures both linear and non-linear trends using kernel tricks. Great when feature relationships aren't obvious.

- **âš¡ XGBoost Regressor**  
  *Why?* Known for speed and performance, XGBoost is an advanced boosting technique that handles missing data, prevents overfitting, and delivers top-tier accuracy in many ML tasks.

## Coming Up Next ðŸš€

We're now ready to train our selected classification models to predict claim approval status. Here's what's coming:

### Evaluation Metrics:
- Accuracy Score
- Classification Report (Precision/Recall/F1)
- Confusion Matrix

> ðŸ” If initial results are underwhelming, no worries! Youâ€™ll have the chance to:
> - ðŸ” Make predictions on new data
> - ðŸ”§ Adjust hyperparameters
> - ðŸ”„ Make predictions on the tuned models
> - ðŸ“ˆ Model Comparison


This modeling phase will help us understand which factors most influence claim approvals and build a predictive system to support insurance decision-making.
